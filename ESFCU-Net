import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Layer
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Conv2D, ReLU, BatchNormalization,Activation, Add
import numpy as np
from tensorflow.keras.layers import Conv2D, Activation, Add, Multiply, Reshape, Permute, Lambda,LeakyReLU


sq1x1 = "squeeze1x1"
exp1x1 = "expand1x1"
exp3x3 = "expand3x3"
relu = "relu_"

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, Reshape, Lambda, Activation, Add, BatchNormalization

def self_attention_module(input_tensor, filters):
    # Self-Attention Module
    # Query, Key, and Value
    Q = Conv2D(filters // 8, kernel_size=(1, 1), strides=(1, 1), padding='same')(input_tensor)
    K = Conv2D(filters // 8, kernel_size=(1, 1), strides=(1, 1), padding='same')(input_tensor)
    V = Conv2D(filters, kernel_size=(1, 1), strides=(1, 1), padding='same')(input_tensor)

    # Reshape for matrix multiplication
    Q_reshaped = Reshape((-1, filters // 8))(Q)
    K_reshaped = Reshape((-1, filters // 8))(K)
    V_reshaped = Reshape((-1, filters))(V)

    # Compute attention scores
    attention_scores = tf.einsum('bij,bkj->bik', Q_reshaped, K_reshaped) / (filters // 8) ** 0.5
    attention_probs = Activation('softmax')(attention_scores)

    # Apply attention to value
    attention_output = tf.einsum('bik,bkj->bij', attention_probs, V_reshaped)
    attention_output = Reshape((input_tensor.shape[1], input_tensor.shape[2], filters))(attention_output)

    # Residual connection and normalization
    attention_output = Add()([input_tensor, attention_output])
    attention_output = BatchNormalization()(attention_output)

    return attention_output


def edge_enhancement_module(input_tensor, filters):
    # Edge Enhancement Convolutional Layer
    x = Conv2D(filters, (3, 3), padding='same', use_bias=False)(input_tensor)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    #
    # x = Conv2D(filters, (3, 3), padding='same', use_bias=False)(x)
    # x = BatchNormalization()(x)
    # x = Activation('LeakyReLU')(x)

    # Add Self-Attention Module
    #attention_output = self_attention_module(x, filters)

    # Combine original input and attention-enhanced features
    #enhanced = Add()([input_tensor, attention_output])
   # enhanced = Activation('relu')(enhanced)


    #return enhanced
    return x

# def fire_module(x, fire_id, squeeze=16, expand=64):
#     s_id = 'fire' + str(fire_id) + '/' # 定义 s_id 作为当前 Fire 模块的标识符，方便层的命名。
#     channel_axis = 3
#     # 创建 squeeze 层，卷积核大小为 1x1，输出通道数为 squeeze 参数定义的值。
#     x = keras.layers.Conv2D(squeeze, (1, 1), padding='valid', use_bias=True,kernel_initializer='he_normal',name=s_id + sq1x1)(x)
#     # 对 squeeze 层的输出应用 keras.layers.BatchNormalization，以稳定和加速训练过程。
#     x = keras.layers.BatchNormalization(axis=-1,momentum=0.95,epsilon=1e-5, name=s_id + sq1x1+'/bn')(x)
#     #x = keras.layers.Activation('relu', name=s_id + relu + sq1x1)(x)
#     # 通过 keras.layers.LeakyReLU 应用 LeakyReLU 激活函数。
#     x=keras.layers.LeakyReLU()(x)
#     # 分别使用 1x1 和 3x3 卷积核在 squeeze 层的输出上创建两个 expand 层。每个 expand 层的输出通道数为 expand//2。（这种设计结合了局部特征和全局特征，增强了网络的表达能力。）对每个 expand 层的输出也应用批归一化和 LeakyReLU 激活函数。
#     left = keras.layers.Conv2D(expand//2, (1, 1), padding='valid',  use_bias=True,kernel_initializer='he_normal',name=s_id + exp1x1)(x)
#     left = keras.layers.BatchNormalization(axis=-1, momentum=0.95,epsilon=1e-5,name=s_id + exp1x1 + '/bn')(left)
#     #left = keras.layers.Activation('relu', name=s_id + relu + exp1x1)(left)
#     left = keras.layers.LeakyReLU()(left)
#     right = keras.layers.Conv2D(expand//2, (3, 3), padding='same', use_bias=True,kernel_initializer='he_normal', name=s_id + exp3x3)(x)
#     right = keras.layers.BatchNormalization(axis=-1,momentum=0.95,epsilon=1e-5,name=s_id + exp3x3 + '/bn')(right)
#     #right = keras.layers.Activation('relu', name=s_id + relu + exp3x3)(right)
#     right = keras.layers.LeakyReLU()(right)
#     # 将左右两个路径的输出在通道轴上进行拼接，从而将通道数增加到 expand。
#     x = keras.layers.concatenate([left, right], axis=channel_axis, name=s_id + 'concat')
#     #print(x)
#     return x


def fire_module(x, fire_id, squeeze=16, expand=64):
    s_id = 'fire' + str(fire_id) + '/'  # 定义 s_id 作为当前 Fire 模块的标识符，方便层的命名
    channel_axis = 3

    # 创建 squeeze 层，卷积核大小为 1x1，输出通道数为 squeeze 参数定义的值
    x = keras.layers.Conv2D(squeeze, (1, 1), padding='valid', use_bias=True,
                            kernel_initializer='he_normal', name=s_id + 'sq1x1')(x)

    # 对 squeeze 层的输出应用 BatchNormalization 层
    x = keras.layers.BatchNormalization(axis=-1, momentum=0.95, epsilon=1e-5,
                                        name=s_id + 'sq1x1_bn')(x)
    x = keras.layers.LeakyReLU()(x)

    # Conv2D (Dilated 3x3) 层
    x_dilated = keras.layers.Conv2D(expand // 4, (3, 3), dilation_rate=3, padding='same',
                                    use_bias=True, kernel_initializer='he_normal',
                                    name=s_id + 'dilated_3x3')(x)
    x_dilated = keras.layers.BatchNormalization(axis=-1, momentum=0.95, epsilon=1e-5,
                                                name=s_id + 'dilated_3x3_bn')(x_dilated)
    x_dilated = keras.layers.LeakyReLU()(x_dilated)

    # Conv2D (Depthwise 3x3) 层
    x_depthwise = keras.layers.DepthwiseConv2D((3, 3), padding='same', depth_multiplier=1,
                                               use_bias=True, kernel_initializer='he_normal',
                                               name=s_id + 'depthwise_3x3')(x)
    x_depthwise = keras.layers.Conv2D(expand // 4, (1, 1), padding='same', use_bias=True,
                                      kernel_initializer='he_normal',
                                      name=s_id + 'depthwise_pointwise_1x1')(x_depthwise)
    x_depthwise = keras.layers.BatchNormalization(axis=-1, momentum=0.95, epsilon=1e-5,
                                                  name=s_id + 'depthwise_3x3_bn')(x_depthwise)
    x_depthwise = keras.layers.LeakyReLU()(x_depthwise)

    # Conv2D (Extended 5x5) 层
    x_extended = keras.layers.Conv2D(expand // 4, (5, 5), padding='same', use_bias=True,
                                     kernel_initializer='he_normal',
                                     name=s_id + 'extended_5x5')(x)
    x_extended = keras.layers.BatchNormalization(axis=-1, momentum=0.95, epsilon=1e-5,
                                                 name=s_id + 'extended_5x5_bn')(x_extended)
    x_extended = keras.layers.LeakyReLU()(x_extended)

    # Conv2D (Pointwise 1x1) 层
    x_pointwise = keras.layers.Conv2D(expand // 4, (1, 1), padding='same', use_bias=True,
                                      kernel_initializer='he_normal',
                                      name=s_id + 'pointwise_1x1')(x)
    x_pointwise = keras.layers.BatchNormalization(axis=-1, momentum=0.95, epsilon=1e-5,
                                                  name=s_id + 'pointwise_1x1_bn')(x_pointwise)
    x_pointwise = keras.layers.LeakyReLU()(x_pointwise)

    # 将不同卷积输出拼接在一起
    x = keras.layers.concatenate([x_dilated, x_depthwise, x_extended, x_pointwise],
                                 axis=channel_axis, name=s_id + 'concat')

    return x


def fire_module_with_attention(input_tensor, fire_id, squeeze, expand, attention_filters):
    x = fire_module(input_tensor, fire_id, squeeze, expand)
    #x = edge_enhancement_module(x, expand)
    x = self_attention_module(x, attention_filters)
    return x

class LoGLayer(Layer):
    def __init__(self, sigma=1.0, **kwargs):
        super(LoGLayer, self).__init__(**kwargs)
        self.sigma = sigma
        self.kernel_size = int(2 * round(3 * sigma) + 1)

    def build(self, input_shape):
        self.channels = input_shape[-1]
        gauss_kernel = self.gaussian_kernel(sigma=self.sigma, kernel_size=self.kernel_size, channels=self.channels)
        self.gauss_kernel = self.add_weight(name='gauss_kernel',
                                            shape=gauss_kernel.shape,
                                            initializer=tf.constant_initializer(gauss_kernel),
                                            trainable=False)
        laplace_kernel = self.laplacian_kernel()
        self.laplace_kernel = self.add_weight(name='laplace_kernel',
                                              shape=laplace_kernel.shape,
                                              initializer=tf.constant_initializer(laplace_kernel),
                                              trainable=False)
        super(LoGLayer, self).build(input_shape)

    def gaussian_kernel(self, sigma, kernel_size, channels):
        ax = np.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.)
        xx, yy = np.meshgrid(ax, ax)
        kernel = np.exp(-0.5 * (xx**2 + yy**2) / sigma**2)
        kernel = kernel / np.sum(kernel)
        kernel = np.repeat(kernel[:, :, np.newaxis], channels, axis=2)
        kernel = kernel[:, :, :, np.newaxis]  # shape [kernel_size, kernel_size, channels, 1]
        return kernel.astype(np.float32)

    def laplacian_kernel(self):
        kernel = np.array([
            [0, 1, 0],
            [1, -4, 1],
            [0, 1, 0]
        ], dtype=np.float32)
        kernel = np.repeat(kernel[:, :, np.newaxis], self.channels, axis=2)
        kernel = kernel[:, :, :, np.newaxis]  # shape [3, 3, channels, 1]
        return kernel

    def call(self, inputs):
        # 高斯模糊
        blurred = tf.nn.depthwise_conv2d(inputs, self.gauss_kernel, strides=[1, 1, 1, 1], padding='SAME')
        # 拉普拉斯算子
        laplace = tf.nn.depthwise_conv2d(blurred, self.laplace_kernel, strides=[1, 1, 1, 1], padding='SAME')
        return laplace

    def compute_output_shape(self, input_shape):
        return input_shape




# def edge_enhancement_module(input_tensor, filters):
#     # 边缘增强卷积层
#     x = Conv2D(filters, (3, 3), padding='same', use_bias=False)(input_tensor)
#     x = BatchNormalization()(x)
#     x = ReLU()(x)
#     # 可选：使用更多的层来进一步提取和增强边缘特征
#     # ...
#
#     # 将原始输入和处理后的特征相加，增强边缘效果
#     enhanced = Add()([input_tensor, x])
#     enhanced = Activation('relu')(enhanced)
#     return enhanced


def ESFCU-NET(input_shape=(256,256,3),dropout=0.3): # 定义网络的输入形状为 (256, 256, 3)，这表明模型期望输入图像为 256x256 像素，具有 3 个颜色通道（RGB）
    """Instantiates the SqueezeNet architecture.
    """
    #filters=[96,128,256,384,512]
    #filters=[64,96,160,256,384]
    filters = [48, 64, 128, 192, 256]
    img_input = keras.layers.Input(shape=input_shape)
    log_output = LoGLayer(sigma=1.0)(img_input)
    # 第一层是一个 2D 卷积层（Conv2D），拥有 filters[0]（48）个过滤器，每个大小为 (3, 3)。步幅设置为 (1, 1)，填充方式为 'same'，意味着输出大小在空间维度上与输入大小相同。你还指定了使用偏置，并使用 He 正态初始化器初始化权重
    x = keras.layers.Conv2D(filters[0], (3, 3), strides=(1, 1), padding='same', use_bias=True,kernel_initializer='he_normal', name='conv1')(log_output)
    #x = keras.layers.Conv2D(filters[0], (3, 3), strides=(1, 1), padding='same',  use_bias=True,kernel_initializer='he_normal',name='conv1')(img_input) #shape=(None, 256, 256, 96)
    # 卷积之后，应用批量归一化，这有助于通过归一化下一层的输入来加速训练并稳定学习过程。
    x=keras.layers.BatchNormalization(axis=-1,momentum=0.95,epsilon=1e-5,name='conv1_bn')(x)
    # 使用 LeakyReLU 激活函数。LeakyReLU 允许当单元不激活且输入小于零时有一个小的梯度，这有助于防止 ReLU 死亡问题。
    x = keras.layers.LeakyReLU()(x)
    # 应用边缘增强模块
    #x = edge_enhancement_module(x, filters[0])
    # 最大池化操作，池化大小为 (2, 2)，步幅为 (2, 2)。这将输入的空间维度减半，增加了过滤器的视野，并减少了后续层的计算负担。
    x = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool1')(x) # shape=(None, 128, 128, 96)

    # 编码器部分通过交替使用 Fire 模块和最大池化层逐步提取特征并降低空间维度。
    # 每个 Fire 模块由一个 squeeze 卷积层和两个 expand 卷积层（1x1 和 3x3）组成，能有效减少参数数量并增强特征提取能力。
    # 将输入 x 通过第一个 Fire 模块（fire_id=2），压缩通道数为16，扩展通道数为 filters[1]（64）。这一步输出的特征图尺寸保持不变，但通道数增加到64。
    f1 = fire_module_with_attention(x, fire_id=2, squeeze=16, expand=filters[1], attention_filters=filters[1]) #shape=(None, 128, 128, 128)
    # 将 Fire 模块1的输出通过第二个 Fire 模块（fire_id=3），设置和上一个相同的squeeze和expand参数。
    f2 = fire_module_with_attention(f1, fire_id=3, squeeze=16, expand=filters[1], attention_filters=filters[1]) #shape=(None, 128, 128, 128)
    # 使用 keras.layers.add 将 Fire 模块2的输出与 Fire 模块1的输出进行元素级相加（残差连接），增强了特征的传递。
    f2= keras.layers.add([f2,f1],name='fire2_3') #shape=(None, 128, 128, 128)
    # 如果设置了 dropout，将在 Fire 模块2的输出上应用 dropout，以减少过拟合。
    if dropout>0:
        f2=keras.layers.Dropout(dropout)(f2)
    # 通过第三个 Fire 模块（fire_id=4），增加squeeze通道数至32，expand通道数至 filters[2]（128）
    f3= fire_module_with_attention(f2, fire_id=4, squeeze=32, expand=filters[2], attention_filters=filters[2])  # shape=(None, 128, 128, 256)
    # 使用最大池化层降低特征图的空间维度，减半至 (64, 64)，通道数为128。
    f4 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool3',padding='same')(f3) #shape=(None, 64, 64, 256)
    # 通过下一个 Fire 模块（fire_id=5）处理，squeeze和expand参数与上一层相同。
    f5 = fire_module_with_attention(f4, fire_id=5, squeeze=32, expand=filters[2], attention_filters=filters[2])  # shape=(None, 64, 64, 256)
    # 使用 keras.layers.add 将此 Fire 模块的输出与最大池化1的输出进行残差连接。
    f5 = keras.layers.add([f5, f4], name='fire4_5')
    if dropout>0:
        f5=keras.layers.Dropout(dropout)(f5)
    # 输入通过下一个 Fire 模块（fire_id=6），这次将 squeeze 通道数增加到 48，expand 通道数增加到 filters[3]（192）。
    f6=fire_module_with_attention(f5, fire_id=6, squeeze=48, expand=filters[3], attention_filters=filters[3]) #shape=(None, 64, 64, 384)
    # 再次使用最大池化层减半空间维度至 (32, 32)，通道数为192。
    f7 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool4', padding='same')(f6)
    # 在最大池化2的输出上应用另一个 Fire 模块（fire_id=7），参数与上一层相同。接着，使用残差连接将此 Fire 模块的输出与最大池化2的输出相加。
    f8 = fire_module_with_attention(f7, fire_id=7, squeeze=48, expand=filters[3], attention_filters=filters[3]) #shape=(None, 32, 32, 384) d
    f9 = keras.layers.add([f7,f8] ,name='fire5_6')   #
    if dropout>0.:
        f9=keras.layers.Dropout(dropout)(f9)
    # 处理经过残差连接后的输出，通过下一个 Fire 模块（fire_id=8），增加 squeeze 通道数至 64，expand 通道数至 filters[4]（256）。
    f9 = fire_module_with_attention(f9, fire_id=8, squeeze=64, expand=filters[4], attention_filters=filters[4])  # shape=(None, 32, 32, 512
    # 最后一次最大池化操作进一步减少空间维度至 (16, 16)，通道数为256。
    f10 = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name='pool8')(f9) #shape=(None, 16, 16, 512)
    #x = fire_module(x, fire_id=8, squeeze=64, expand=256) #13, 13, 512
    # 最终，通过最后一个 Fire 模块（fire_id=9）处理，squeeze 和 expand 参数与上一层相同，得到最终的特征表示。
    f10 = fire_module_with_attention(f10, fire_id=9, squeeze=64, expand=filters[4], attention_filters=filters[4]) # shape=(None, 16, 16, 512)
    #if dropout>0.:
    #    f10=keras.layers.Dropout(dropout/2)(f10)

    # 使用转置卷积层将 f10 的特征图上采样，将空间维度增加到 (32, 32)，通道数为 filters[4]（256）。
    up5=keras.layers.Conv2DTranspose(filters[4],kernel_size=1,strides=(2,2),padding='same',name='up5_conv2d_trans')(f10) #shape=(None, 32, 32, 512)
    #up5 = edge_enhancement_module(up5, filters[4])
    # 将上采样后的特征图与 f9（来自编码器的对应阶段）进行元素级相加，以引入编码器阶段的上下文信息。
    up5d=keras.layers.Add()([up5,f9])
    up5d = keras.layers.LeakyReLU()(up5d)
    # 处理经过跳跃连接后的特征图，通过 Fire 模块（fire_id=10），再次进行特征提取和压缩/扩展操作，以进一步精细化特征表示。
    up5d = fire_module_with_attention(up5d, fire_id=10, squeeze=48, expand=filters[3],attention_filters=filters[3]) #shape=(None, 32, 32, 384)
    up5d = keras.layers.BatchNormalization()(up5d)

    # 重复上述过程，使用转置卷积层将特征图上采样到 (64, 64)，并通过跳跃连接结合来自编码器的 f6 特征图，然后通过 Fire 模块（fire_id=11）进行处理。
    up4 = keras.layers.Conv2DTranspose(filters[3], kernel_size=1, strides=(2, 2), padding='same',name='up4_conv2d_trans')(up5d) #shape=(None, 64, 64, 384)
    up4d = keras.layers.Add()([up4, f6])
    up4d = keras.layers.LeakyReLU()(up4d)
    up4d = fire_module_with_attention(up4d, fire_id=11, squeeze=32, expand=filters[2],attention_filters=filters[2])
    up4d = keras.layers.BatchNormalization()(up4d)
    up4d = keras.layers.Dropout(0.3)(up4d)

    # 继续上采样和跳跃连接的过程，这次将特征图上采样到 (128, 128)，并结合来自编码器的 f3 特征图，通过 Fire 模块（fire_id=12）进行进一步的特征提取和优化。
    up3 = keras.layers.Conv2DTranspose(filters[2], kernel_size=1, strides=(2, 2), padding='same',name='up3_conv2d_trans')(up4d) #shape=(None, 128, 128, 256)
    up3d = keras.layers.Add()([up3, f3])
    up3d = keras.layers.LeakyReLU()(up3d)
    up3d = fire_module_with_attention(up3d, fire_id=12, squeeze=16, expand=filters[1],attention_filters=filters[1])
    #up3d = fire_module(up3d, fire_id=13, squeeze=16, expand=128)

    # 最后一个上采样阶段，使用转置卷积层将特征图上采样并通过一个 3x3 卷积层进行最后的特征提取，随后应用批量归一化和 LeakyReLU 激活函数。
    up20 = keras.layers.Conv2DTranspose(filters[1], kernel_size=1, strides=(2, 2), padding='same',name='up2_conv2d_trans')(up3d)
    up2 = keras.layers.Conv2D(filters[0], (3, 3), strides=(1, 1), padding='same', use_bias=True, kernel_initializer='he_normal', name='conv_up2')(up20)
    up2 = keras.layers.BatchNormalization(axis=-1, momentum=0.95, epsilon=1e-5, name='conv_up2_bn')(up2)
    up2 = keras.layers.LeakyReLU()(up2)

    # 通过多尺度的特征图生成多个分割图，然后将这些分割图与原始输入图像连接，最终生成综合的分割结果。
    # d1、d2、d3、d4：这些层分别从不同层次的上采样特征图 up2、up20、up3d 和 up4d 生成分割图。
    # 首先，使用 (3, 3) 卷积核生成特征图，然后通过 sigmoid 激活函数转换为二值分割图（d11、d22、d33、d44）。
    # sigmoid 函数将特征图中的每个像素值压缩到 [0, 1] 范围内，适用于二分类任务，其中 1 表示前景，0 表示背景。
    d1 = keras.layers.Conv2D(1, (3, 3), padding="same", activation=None, use_bias=False)(up2)
    d11 = keras.layers.Activation('sigmoid', name='d1')(d1)

    d2 = keras.layers.Conv2D(1, (3, 3), padding="same", activation=None, use_bias=False)(up20)
    d22 = keras.layers.Activation('sigmoid', name='d2')(d2)

    # 在生成 d3 和 d4 之前，你先通过转置卷积层对 up3d 和 up4d 进行上采样，使它们的空间维度与目标分割图尺寸一致。
    d3 = keras.layers.Conv2DTranspose(filters[1], kernel_size=1, strides=(2, 2), padding='same',name='d3_conv2d_trans')(up3d)
    d3 = keras.layers.Conv2D(1, (3, 3), padding="same", activation=None, use_bias=False)(d3)
    d33 = keras.layers.Activation('sigmoid', name='d3')(d3)

    d4 = keras.layers.Conv2DTranspose(filters[2], kernel_size=3, strides=(4, 4), padding='same',name='d4_conv2d_trans')(up4d)
    #d4 = keras.layers.Conv2DTranspose(256, kernel_size=3, strides=(2, 2), padding='same')(d4)
    #d4 = keras.layers.UpSampling2D(size=(2, 2))(d4)
    d4 = keras.layers.Conv2D(1, (3, 3), padding="same", activation=None, use_bias=False)(d4)
    d44 = keras.layers.Activation('sigmoid', name='d4')(d4)

    d5 = keras.layers.Conv2DTranspose(filters[3], kernel_size=1, strides=(8,8), padding='same',name='d5_conv2d_trans')(up5d)
    # d5 = keras.layers.Conv2DTranspose(384, kernel_size=3, strides=(2, 2), padding='same')(d5) #1327488
    # d5 = keras.layers.Conv2DTranspose(filters[3], kernel_size=1, strides=(2, 2), padding='same')(d5) #1327488
    #d5=keras.layers.UpSampling2D(size=(2,2))(d5)
    d5 = keras.layers.Conv2D(1, (3, 3), padding="same", activation=None, use_bias=False)(d5)
    d55 = keras.layers.Activation('sigmoid', name='d5')(d5)

    # 将所有单独的分割图 d1、d2、d3、d4 与原始输入图像 img_input 进行连接，形成一个深度融合的特征图。然后，通过一个 (3, 3) 卷积层进一步提取特征，并通过 sigmoid 激活函数生成最终的二值分割图 d。
    d = keras.layers.concatenate([d1, d2, d3, d4,d5,  img_input])
    d = keras.layers.Conv2D(1, kernel_size=3, activation=None, padding='same', use_bias=False)(d)
    d = keras.layers.Activation('sigmoid', name='d')(d)

    # 最后，将输入图像 img_input 与所有分割图（d、d11、d22、d33、d44）作为输出
    model = keras.models.Model(inputs=img_input, outputs=[d, d11, d22, d33, d44,d55])

    return model


if __name__ == "__main__":
    #Total params: 1,767,464
    model=ESFCU-NET(input_shape=(256,256,3),dropout=0.3)
    model.summary()
    print(f"Total number of layers: {len(model.layers)}")
    #keras.utils.plot_model(model, 'ESFCU-NET.png', show_shapes=True)
    print("模型加载成功")
